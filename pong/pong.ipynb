{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on article: https://arxiv.org/pdf/1602.01783v1.pdf\n",
    "And code https://gist.github.com/kkweon/67bc50fed5d4dc08b844d45479bebe75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from threading import Thread\n",
    "from time import sleep\n",
    "from os.path import dirname\n",
    "from glob import glob\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possible_movements = np.array([1, 2, 3, 4, 5])\n",
    "output_dim = len(possible_movements)\n",
    "first_number = possible_movements[0]\n",
    "procces_image_size = [80, 80, 1]\n",
    "max_episode_number = 1000\n",
    "\n",
    "save_path = \"some_model/model\"\n",
    "load_path = \"some_model/model\"\n",
    "n_threads = 3\n",
    "\n",
    "monitor = True\n",
    "monitor_name = \"monitor\"\n",
    "\n",
    "terminate_time = 2 # Program will finish after terminate_time hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for running pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def procces_image(image):\n",
    "    \"\"\"Returns a preprocessed image original pong picture is 210x160x3\n",
    "    (1) Crop image (top and bottom), and resize it (delete every other pixel)\n",
    "    (2) Remove background & grayscale\n",
    "\n",
    "    Args:\n",
    "        image (3-D array): (210, 160, 3)\n",
    "\n",
    "    Returns:\n",
    "        image (3-D array): (80, 80, 1)\n",
    "    \"\"\"\n",
    "    image = image[35:195:2, ::2] # crop image on 80, 80\n",
    "    image = ((image[:,:,0] != 144) & (image[:,:,0] != 119)).astype('float32')\n",
    "    image = np.expand_dims(image, axis=2)\n",
    "    return image\n",
    "\n",
    "def discount_reward(rewards, gamma=0.99):\n",
    "    \"\"\"Returns discounted rewards\n",
    "\n",
    "    Args:\n",
    "        rewards (1-D array): Reward array\n",
    "        gamma (float): Discounted rate\n",
    "\n",
    "    Returns:\n",
    "        discounted_rewards: same shape as `rewards`\n",
    "\n",
    "    Notes:\n",
    "        In Pong, when the reward can be {-1, 0, 1}.\n",
    "\n",
    "        However, when the reward is either -1 or 1,\n",
    "        it means the game has been reset.\n",
    "\n",
    "        Therefore, it's necessaray to reset `running_add` to 0\n",
    "        whenever the reward is nonzero\n",
    "\n",
    "        This will introduce a bias in our gradients: we will train on more steps,\n",
    "        but we will introduce a systematic error in the gradient.\n",
    "    \"\"\"\n",
    "    advantage = np.zeros_like(rewards)\n",
    "    running_add = 0\n",
    "    for t in range(len(rewards)-1, -1, -1):\n",
    "        if rewards[t] != 0:\n",
    "            running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + rewards[t]\n",
    "        advantage[t] = running_add\n",
    "    advantage -= advantage.mean()\n",
    "    advantage /= advantage.std()\n",
    "#     advantage /= advantage.std() + 1e-8\n",
    "    return advantage\n",
    "\n",
    "\n",
    "def copy_src_to_dst(from_scope, to_scope):\n",
    "    \"\"\"Creates a copy variable weights operation\n",
    "    Args:\n",
    "        from_scope (str): The name of scope to copy from\n",
    "            It should be \"global\"\n",
    "        to_scope (str): The name of scope to copy to\n",
    "            It should be \"thread-{}\"\n",
    "    Returns:\n",
    "        list: Each element is a copy operation\n",
    "    \"\"\"\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var, to_var in zip(from_vars, to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(Thread):\n",
    "\n",
    "    def __init__(self, session, env, coord, name, global_network, input_shape=[80, 80, 1], output_dim=3, logdir=None):\n",
    "        \"\"\"Agent worker thread\n",
    "        Args:\n",
    "            session (tf.Session): Tensorflow session needs to be shared\n",
    "            env (gym.env): Gym environment\n",
    "            coord (tf.train.Coordinator): Tensorflow Queue Coordinator\n",
    "            name (str): Name of this worker\n",
    "            global_network (A3CNetwork): Global network that needs to be updated\n",
    "            input_shape (list): Required for local A3CNetwork (H, W, C)\n",
    "            output_dim (int): Number of actions\n",
    "            logdir (str, optional): If logdir is given, will write summary\n",
    "        \"\"\"\n",
    "        super(Agent, self).__init__()\n",
    "        self.local = A3CNetwork(name, input_shape, output_dim, logdir)\n",
    "        self.global_to_local = copy_src_to_dst(\"global\", name)\n",
    "        self.global_network = global_network\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.output_dim = output_dim\n",
    "        self.env = env\n",
    "        self.sess = session\n",
    "        self.coord = coord\n",
    "        self.name = name\n",
    "        self.logdir = logdir\n",
    "\n",
    "    def play_episode(self, episode_number):\n",
    "        # Update local weights\n",
    "        self.sess.run(self.global_to_local)\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        state = self.env.reset()\n",
    "        state = procces_image(state)\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        time_step = 0\n",
    "        time_max = np.random.randint(15, 25)\n",
    "        action_repeat = 1\n",
    "        old_states = 1\n",
    "        states_old = [np.zeros_like(state) for i in range(old_states)]\n",
    "        states_old[-1] = state\n",
    "        state_diff = np.zeros_like(state)\n",
    "        while not done:\n",
    "\n",
    "            action = self.choose_action(state_diff)\n",
    "            sum_reward = 0\n",
    "            for i in range(action_repeat):\n",
    "                state_new, reward, done, _ = self.env.step(action)\n",
    "                sum_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            state_new = procces_image(state_new)\n",
    "            total_reward += sum_reward\n",
    "\n",
    "            states.append(state_diff)\n",
    "            actions.append(action)\n",
    "            rewards.append(sum_reward)\n",
    "\n",
    "            state_diff = state_new - np.sum(states_old, axis=0)\n",
    "\n",
    "            for i, j in zip(range(old_states-1), range(1, old_states)):\n",
    "                states_old[i] = states_old[j]\n",
    "            states_old[-1] = state_new\n",
    "\n",
    "            if sum_reward != 0 or done:\n",
    "                time_step += 1\n",
    "\n",
    "                if time_step >= time_max or done:\n",
    "                    self.train(states, actions, rewards)\n",
    "                    self.sess.run(self.global_to_local)\n",
    "                    states, actions, rewards = [], [], []\n",
    "                    time_step = 0\n",
    "\n",
    "#             time_step += 1\n",
    "\n",
    "#             if time_step >= time_max or done:\n",
    "#                 self.train(states, actions, rewards)\n",
    "#                 self.sess.run(self.global_to_local)\n",
    "#                 states, actions, rewards = [], [], []\n",
    "#                 time_step = 0\n",
    "\n",
    "        print(\"Agent -> {}, reward = {}, episode = {}\".format(self.name, total_reward, episode_number))\n",
    "\n",
    "    def run(self):\n",
    "        episode_number = 0\n",
    "        while not self.coord.should_stop() or episode_number > max_episode_number:\n",
    "            self.play_episode(episode_number)\n",
    "            episode_number += 1\n",
    "        raise SystemExit\n",
    "\n",
    "    def choose_action(self, states):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            states (proccesed image): (H, W, 1)/(input_shape)\n",
    "        \"\"\"\n",
    "        states = np.reshape(states, [-1] + self.input_shape)\n",
    "        feed = { self.local.states: states }\n",
    "\n",
    "        action = self.sess.run(self.local.action_prob, feed)\n",
    "        action = np.squeeze(action)\n",
    "\n",
    "        return np.random.choice(np.arange(self.output_dim) + 1, p=action)\n",
    "\n",
    "    def train(self, states, actions, rewards):\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions) - first_number # in atari we don't start from 0\n",
    "        rewards = np.array(rewards)\n",
    "\n",
    "        # Calculate value for each state\n",
    "        feed = { self.local.states: states }\n",
    "        values = self.sess.run(self.local.values, feed)\n",
    "\n",
    "        # Smears rewards over all actions (we don't know exact which action did good move)\n",
    "        rewards = discount_reward(rewards, gamma=0.99)\n",
    "\n",
    "        advantage = rewards - values\n",
    "        advantage -= np.mean(advantage)\n",
    "        advantage /= np.std(advantage) + 1e-8\n",
    "\n",
    "        feed = {\n",
    "            self.local.states: states,\n",
    "            self.local.actions: actions,\n",
    "            self.local.rewards: rewards,\n",
    "            self.local.advantage: advantage\n",
    "        }\n",
    "\n",
    "        gradients = self.sess.run(self.local.gradients, feed)\n",
    "\n",
    "        feed = []\n",
    "        for (grad, _), (placeholder, _) in zip(gradients, self.global_network.gradients_placeholders):\n",
    "            feed.append((placeholder, grad))\n",
    "\n",
    "        feed = dict(feed)\n",
    "        self.sess.run(self.global_network.apply_gradients, feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class A3CNetwork(object):\n",
    "\n",
    "    def __init__(self, name, input_shape, output_dim, logdir=None):\n",
    "        \"\"\"Network structure is defined here\n",
    "        Args:\n",
    "            name (str): The name of scope\n",
    "            input_shape (list): The shape of input image [H, W, C]\n",
    "            output_dim (int): Number of actions\n",
    "            logdir (str, optional): directory to save summaries\n",
    "                TODO: create a summary op\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name):\n",
    "            self.states = tf.placeholder(tf.float32, shape=[None] + input_shape, name=\"states\")\n",
    "            self.actions = tf.placeholder(tf.uint8, shape=[None], name=\"actions\")\n",
    "            self.rewards = tf.placeholder(tf.float32, shape=[None], name=\"rewards\")\n",
    "            self.advantage = tf.placeholder(tf.float32, shape=[None], name=\"advantage\")\n",
    "\n",
    "            action_onehot = tf.one_hot(self.actions, output_dim, name=\"action_onehot\")\n",
    "            net = self.states\n",
    "\n",
    "            with tf.variable_scope(\"layer1\"):\n",
    "                net = tf.layers.conv2d(net,\n",
    "                                       filters=16,\n",
    "                                       kernel_size=(8, 8),\n",
    "                                       strides=(4, 4),\n",
    "                                       name=\"conv\")\n",
    "                net = tf.nn.relu(net, name=\"relu\")\n",
    "\n",
    "            with tf.variable_scope(\"layer2\"):\n",
    "                net = tf.layers.conv2d(net,\n",
    "                                       filters=32,\n",
    "                                       kernel_size=(4, 4),\n",
    "                                       strides=(2, 2),\n",
    "                                       name=\"conv\")\n",
    "                net = tf.nn.relu(net, name=\"relu\")\n",
    "\n",
    "            with tf.variable_scope(\"fc1\"):\n",
    "                net = tf.contrib.layers.flatten(net)\n",
    "                net = tf.layers.dense(net, 256, name='dense')\n",
    "                net = tf.nn.relu(net, name='relu')\n",
    "\n",
    "            # actor network\n",
    "            actions = tf.layers.dense(net, output_dim, name=\"final_fc\")\n",
    "            self.action_prob = tf.nn.softmax(actions, name=\"action_prob\")\n",
    "            single_action_prob = tf.reduce_sum(self.action_prob * action_onehot, axis=1)\n",
    "\n",
    "            entropy = - self.action_prob * tf.log(self.action_prob + 1e-7)\n",
    "            entropy = tf.reduce_sum(entropy, axis=1)\n",
    "\n",
    "            log_action_prob = tf.log(single_action_prob + 1e-7)\n",
    "            maximize_objective = log_action_prob * self.advantage\n",
    "#             maximize_objective = log_action_prob * self.advantage + entropy * 0.005\n",
    "            self.actor_loss = - tf.reduce_mean(maximize_objective)\n",
    "\n",
    "            # value network\n",
    "            self.values = tf.squeeze(tf.layers.dense(net, 1, name=\"values\"))\n",
    "            self.value_loss = tf.losses.mean_squared_error(labels=self.rewards,\n",
    "                                                           predictions=self.values)\n",
    "\n",
    "            self.total_loss = self.actor_loss + self.value_loss * .5\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(learning_rate=0.0001, decay=.99)\n",
    "\n",
    "        var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
    "        self.gradients = self.optimizer.compute_gradients(self.total_loss, var_list)\n",
    "        self.gradients_placeholders = []\n",
    "\n",
    "        for grad, var in self.gradients:\n",
    "            self.gradients_placeholders.append((tf.placeholder(var.dtype, shape=var.get_shape()), var))\n",
    "        self.apply_gradients = self.optimizer.apply_gradients(self.gradients_placeholders)\n",
    "\n",
    "        if logdir:\n",
    "            loss_summary = tf.summary.scalar(\"total_loss\", self.total_loss)\n",
    "            value_summary = tf.summary.histogram(\"values\", self.values)\n",
    "\n",
    "            self.summary_op = tf.summary.merge([loss_summary, value_summary])\n",
    "            self.summary_writer = tf.summary.FileWriter(logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_pong():\n",
    "    try:\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.InteractiveSession()\n",
    "        coord = tf.train.Coordinator()\n",
    "\n",
    "        global_network = A3CNetwork(name=\"global\", input_shape=procces_image_size, output_dim=output_dim)\n",
    "\n",
    "        thread_list = []\n",
    "        env_list = []\n",
    "\n",
    "        for id in range(n_threads):\n",
    "            env = gym.make(\"Pong-v0\")\n",
    "\n",
    "            if id == 0 and monitor:\n",
    "                env = gym.wrappers.Monitor(env, monitor_name, force=True)\n",
    "\n",
    "            single_agent = Agent(env=env, session=sess, coord=coord,\n",
    "                                 name=\"thread_{}\".format(id), global_network=global_network,\n",
    "                                 input_shape=procces_image_size, output_dim=output_dim)\n",
    "\n",
    "            thread_list.append(single_agent)\n",
    "            env_list.append(env)\n",
    "\n",
    "        if tf.train.get_checkpoint_state(dirname(load_path)):\n",
    "            var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"global\")\n",
    "            saver = tf.train.Saver(var_list=var_list)\n",
    "            saver.restore(sess, load_path)\n",
    "            print(\"Load from\", load_path)\n",
    "            print(\"Model restored to global\")\n",
    "        else:\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "            print(\"No model is found\")\n",
    "\n",
    "        for t in thread_list:\n",
    "            t.start()\n",
    "\n",
    "        print(\"Keyboard or System interupt to close\")\n",
    "        sleep(60*60*terminate_time)\n",
    "        raise SystemExit\n",
    "        # coord.wait_for_stop()\n",
    "\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"global\")\n",
    "        saver = tf.train.Saver(var_list=var_list)\n",
    "        saver.save(sess, save_path)\n",
    "        print()\n",
    "        print(\"=\" * 10)\n",
    "        print('Checkpoint Saved to {}'.format(save_path))\n",
    "        print(\"=\" * 10)\n",
    "\n",
    "        print(\"Closing threads\")\n",
    "        coord.request_stop()\n",
    "        coord.join(thread_list)\n",
    "\n",
    "        print(\"Closing environments\")\n",
    "        for env in env_list:\n",
    "            env.close()\n",
    "\n",
    "        sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-29 11:37:35,237] Making new env: Pong-v0\n",
      "[2017-06-29 11:37:35,424] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-06-29 11:37:35,811] Making new env: Pong-v0\n",
      "[2017-06-29 11:37:36,317] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from dim5_model_v0/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-29 11:37:37,067] Restoring parameters from dim5_model_v0/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Load from', 'dim5_model_v0/model')\n",
      "Model restored to global\n",
      "Keyboard or System interupt to close\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-29 11:37:37,270] Starting new video recorder writing to /home/ivan/Dropbox/Faks/5_godina/nn_assignments/pong/tmp_monitor/openaigym.video.0.32278.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent -> thread_1, reward = 2.0, episode = 0\n",
      "Agent -> thread_2, reward = -5.0, episode = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-29 11:38:00,741] Starting new video recorder writing to /home/ivan/Dropbox/Faks/5_godina/nn_assignments/pong/tmp_monitor/openaigym.video.0.32278.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent -> thread_0, reward = -3.0, episode = 0\n",
      "Agent -> thread_1, reward = 3.0, episode = 1\n",
      "Agent -> thread_2, reward = -1.0, episode = 1\n",
      "Agent -> thread_0, reward = -3.0, episode = 1\n",
      "Agent -> thread_1, reward = -8.0, episode = 2\n",
      "Agent -> thread_2, reward = -5.0, episode = 2\n",
      "Agent -> thread_0, reward = -5.0, episode = 2\n",
      "()\n",
      "==========\n",
      "Checkpoint Saved to tmp_model/model\n",
      "==========\n",
      "Closing threads\n",
      "Agent -> thread_1, reward = -8.0, episode = 3\n",
      "Agent -> thread_2, reward = -9.0, episode = 3\n",
      "Agent -> thread_0, reward = -4.0, episode = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-29 11:39:23,269] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/ivan/Dropbox/Faks/5_godina/nn_assignments/pong/tmp_monitor')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing environments\n"
     ]
    }
   ],
   "source": [
    "# possible_movements = np.array([1, 2, 3])\n",
    "possible_movements = np.array([1, 2, 3, 4, 5])\n",
    "output_dim = len(possible_movements)\n",
    "first_number = possible_movements[0]\n",
    "procces_image_size = [80, 80, 1]\n",
    "max_episode_number = 1000\n",
    "\n",
    "save_path = \"tmp_model/model\"\n",
    "load_path = \"dim5_model_v0/model\"\n",
    "n_threads = 3\n",
    "\n",
    "monitor = True\n",
    "monitor_name = \"tmp_monitor\"\n",
    "\n",
    "terminate_time = 2 # Program will finish after terminate_time hours\n",
    "\n",
    "run_pong()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    /openaigym.video.0.32278.video000000.mp4 <br>\n",
       "    <video width=\"320\" height=\"240\" controls>\n",
       "      <source src=\"tmp_monitor/openaigym.video.0.32278.video000000.mp4\" type=\"video/mp4\">\n",
       "      Your browser does not support the video tag.\n",
       "    </video><br>\n",
       "    \n",
       "    /openaigym.video.0.32278.video000001.mp4 <br>\n",
       "    <video width=\"320\" height=\"240\" controls>\n",
       "      <source src=\"tmp_monitor/openaigym.video.0.32278.video000001.mp4\" type=\"video/mp4\">\n",
       "      Your browser does not support the video tag.\n",
       "    </video><br>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def play_videos():\n",
    "    video_template = \"\"\"\n",
    "    %s <br>\n",
    "    <video width=\"320\" height=\"240\" controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "      Your browser does not support the video tag.\n",
    "    </video><br>\n",
    "    \"\"\"\n",
    "\n",
    "    videos = []\n",
    "\n",
    "    for f in glob(monitor_name + \"*/*.mp4\"):\n",
    "        videos.append(video_template % (f[len(monitor_name):],f))\n",
    "\n",
    "    return HTML(''.join(videos))\n",
    "\n",
    "play_videos()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-29 13:52:36,190] Making new env: Pong-v0\n",
      "[2017-06-29 13:52:36,360] Clearing 8 monitor files from previous run (because force=True was provided)\n",
      "[2017-06-29 13:52:36,811] Making new env: Pong-v0\n",
      "[2017-06-29 13:52:37,254] Making new env: Pong-v0\n",
      "[2017-06-29 13:52:37,693] Making new env: Pong-v0\n",
      "[2017-06-29 13:52:38,288] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from dim5_model_v0/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-29 13:52:38,770] Restoring parameters from dim5_model_v0/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Load from', 'dim5_model_v0/model')\n",
      "Model restored to global\n",
      "Keyboard or System interupt to close\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-29 13:52:39,016] Starting new video recorder writing to /home/ivan/Dropbox/Faks/5_godina/nn_assignments/pong/tmp2_monitor/openaigym.video.3.8333.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent -> thread_4, reward = -4.0, episode = 0\n",
      "Agent -> thread_1, reward = -8.0, episode = 0\n",
      "Agent -> thread_2, reward = -2.0, episode = 0\n",
      "Agent -> thread_3, reward = -2.0, episode = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-29 13:53:21,351] Starting new video recorder writing to /home/ivan/Dropbox/Faks/5_godina/nn_assignments/pong/tmp2_monitor/openaigym.video.3.8333.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent -> thread_0, reward = -4.0, episode = 0\n",
      "Agent -> thread_4, reward = -10.0, episode = 1\n",
      "Agent -> thread_1, reward = -7.0, episode = 1\n",
      "Agent -> thread_2, reward = -2.0, episode = 1\n",
      "Agent -> thread_3, reward = 3.0, episode = 1\n",
      "Agent -> thread_0, reward = -10.0, episode = 1\n",
      "Agent -> thread_2, reward = -13.0, episode = 2\n",
      "Agent -> thread_4, reward = 1.0, episode = 2\n",
      "Agent -> thread_1, reward = 1.0, episode = 2\n",
      "Agent -> thread_3, reward = -7.0, episode = 2\n",
      "Agent -> thread_0, reward = -1.0, episode = 2\n",
      "Agent -> thread_4, reward = -7.0, episode = 3\n",
      "Agent -> thread_2, reward = -3.0, episode = 3\n",
      "Agent -> thread_1, reward = -7.0, episode = 3\n",
      "Agent -> thread_3, reward = -4.0, episode = 3\n",
      "Agent -> thread_0, reward = -7.0, episode = 3\n",
      "Agent -> thread_2, reward = -7.0, episode = 4\n",
      "Agent -> thread_4, reward = -8.0, episode = 4\n",
      "Agent -> thread_1, reward = -3.0, episode = 4\n",
      "Agent -> thread_0, reward = -8.0, episode = 4\n",
      "Agent -> thread_2, reward = -10.0, episode = 5\n",
      "Agent -> thread_3, reward = -2.0, episode = 4\n",
      "Agent -> thread_4, reward = -9.0, episode = 5\n",
      "Agent -> thread_1, reward = -7.0, episode = 5\n",
      "Agent -> thread_0, reward = -7.0, episode = 5\n",
      "Agent -> thread_3, reward = -10.0, episode = 5\n",
      "Agent -> thread_2, reward = -5.0, episode = 6\n",
      "Agent -> thread_1, reward = -8.0, episode = 6\n",
      "Agent -> thread_0, reward = -10.0, episode = 6\n",
      "Agent -> thread_4, reward = -3.0, episode = 6\n",
      "Agent -> thread_1, reward = -8.0, episode = 7\n",
      "Agent -> thread_2, reward = -9.0, episode = 7\n",
      "Agent -> thread_3, reward = 1.0, episode = 6\n",
      "Agent -> thread_4, reward = -10.0, episode = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-29 13:57:22,394] Starting new video recorder writing to /home/ivan/Dropbox/Faks/5_godina/nn_assignments/pong/tmp2_monitor/openaigym.video.3.8333.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent -> thread_0, reward = -3.0, episode = 7\n",
      "Agent -> thread_2, reward = -10.0, episode = 8\n",
      "Agent -> thread_1, reward = -7.0, episode = 8\n",
      "Agent -> thread_3, reward = -1.0, episode = 7\n",
      "Agent -> thread_4, reward = -9.0, episode = 8\n",
      "Agent -> thread_0, reward = 3.0, episode = 8\n",
      "Agent -> thread_2, reward = -8.0, episode = 9\n",
      "Agent -> thread_1, reward = -4.0, episode = 9\n",
      "Agent -> thread_3, reward = -8.0, episode = 8\n",
      "Agent -> thread_4, reward = -3.0, episode = 9\n",
      "Agent -> thread_0, reward = -9.0, episode = 9\n",
      "Agent -> thread_2, reward = 3.0, episode = 10\n",
      "Agent -> thread_1, reward = -5.0, episode = 10\n",
      "Agent -> thread_3, reward = -4.0, episode = 9\n",
      "Agent -> thread_4, reward = -4.0, episode = 10\n",
      "Agent -> thread_2, reward = -9.0, episode = 11\n",
      "Agent -> thread_0, reward = -2.0, episode = 10\n",
      "Agent -> thread_3, reward = -6.0, episode = 10\n",
      "Agent -> thread_1, reward = -3.0, episode = 11\n",
      "Agent -> thread_4, reward = -3.0, episode = 11\n",
      "Agent -> thread_0, reward = -10.0, episode = 11\n",
      "Agent -> thread_2, reward = -9.0, episode = 12\n",
      "Agent -> thread_1, reward = -7.0, episode = 12\n",
      "Agent -> thread_3, reward = -4.0, episode = 11\n",
      "Agent -> thread_4, reward = -10.0, episode = 12\n",
      "Agent -> thread_2, reward = -9.0, episode = 13\n",
      "Agent -> thread_0, reward = 2.0, episode = 12\n",
      "Agent -> thread_4, reward = -10.0, episode = 13\n",
      "Agent -> thread_3, reward = -9.0, episode = 12\n",
      "Agent -> thread_1, reward = 3.0, episode = 13\n",
      "Agent -> thread_2, reward = -7.0, episode = 14\n",
      "Agent -> thread_0, reward = -2.0, episode = 13\n",
      "Agent -> thread_4, reward = 3.0, episode = 14\n",
      "Agent -> thread_3, reward = -8.0, episode = 13\n",
      "Agent -> thread_1, reward = -8.0, episode = 14\n",
      "Agent -> thread_2, reward = 4.0, episode = 15\n",
      "Agent -> thread_0, reward = -8.0, episode = 14\n",
      "Agent -> thread_3, reward = -9.0, episode = 14\n",
      "Agent -> thread_4, reward = -5.0, episode = 15\n",
      "Agent -> thread_1, reward = -7.0, episode = 15\n",
      "Agent -> thread_2, reward = -8.0, episode = 16\n",
      "Agent -> thread_0, reward = -8.0, episode = 15\n",
      "Agent -> thread_1, reward = -10.0, episode = 16\n",
      "Agent -> thread_4, reward = -8.0, episode = 16\n",
      "Agent -> thread_3, reward = -3.0, episode = 15\n",
      "Agent -> thread_2, reward = -7.0, episode = 17\n",
      "Agent -> thread_0, reward = -6.0, episode = 16\n",
      "Agent -> thread_1, reward = -7.0, episode = 17\n",
      "Agent -> thread_3, reward = -9.0, episode = 16\n",
      "Agent -> thread_4, reward = -7.0, episode = 17\n",
      "Agent -> thread_2, reward = -8.0, episode = 18\n",
      "Agent -> thread_3, reward = -5.0, episode = 17\n",
      "Agent -> thread_0, reward = 2.0, episode = 17\n",
      "Agent -> thread_1, reward = -3.0, episode = 18\n",
      "Agent -> thread_4, reward = -8.0, episode = 18\n",
      "Agent -> thread_2, reward = -8.0, episode = 19\n",
      "Agent -> thread_3, reward = -3.0, episode = 18\n",
      "Agent -> thread_0, reward = -4.0, episode = 18\n",
      "Agent -> thread_1, reward = -6.0, episode = 19\n",
      "Agent -> thread_4, reward = -7.0, episode = 19\n",
      "Agent -> thread_2, reward = -3.0, episode = 20\n",
      "Agent -> thread_3, reward = -5.0, episode = 19\n",
      "Agent -> thread_0, reward = -6.0, episode = 19\n",
      "Agent -> thread_1, reward = -1.0, episode = 20\n",
      "Agent -> thread_4, reward = -2.0, episode = 20\n",
      "Agent -> thread_2, reward = -11.0, episode = 21\n",
      "Agent -> thread_0, reward = -6.0, episode = 20\n",
      "Agent -> thread_3, reward = -5.0, episode = 20\n",
      "Agent -> thread_1, reward = 5.0, episode = 21\n",
      "Agent -> thread_4, reward = -4.0, episode = 21\n",
      "Agent -> thread_2, reward = -5.0, episode = 22\n",
      "Agent -> thread_0, reward = -10.0, episode = 21\n",
      "Agent -> thread_1, reward = -4.0, episode = 22\n",
      "Agent -> thread_3, reward = -3.0, episode = 21\n",
      "Agent -> thread_4, reward = -7.0, episode = 22\n",
      "Agent -> thread_2, reward = -3.0, episode = 23\n",
      "Agent -> thread_0, reward = -9.0, episode = 22\n",
      "Agent -> thread_1, reward = -5.0, episode = 23\n",
      "Agent -> thread_4, reward = -4.0, episode = 23\n",
      "Agent -> thread_3, reward = -2.0, episode = 22\n",
      "Agent -> thread_2, reward = -6.0, episode = 24\n",
      "Agent -> thread_0, reward = -3.0, episode = 23\n",
      "Agent -> thread_2, reward = -14.0, episode = 25\n",
      "Agent -> thread_3, reward = -4.0, episode = 23\n",
      "Agent -> thread_1, reward = -4.0, episode = 24\n",
      "Agent -> thread_4, reward = -3.0, episode = 24\n",
      "Agent -> thread_0, reward = -8.0, episode = 24\n",
      "Agent -> thread_1, reward = -8.0, episode = 25\n",
      "Agent -> thread_3, reward = -9.0, episode = 24\n",
      "Agent -> thread_4, reward = -2.0, episode = 25\n",
      "Agent -> thread_2, reward = -1.0, episode = 26\n",
      "Agent -> thread_0, reward = -8.0, episode = 25\n",
      "Agent -> thread_1, reward = -8.0, episode = 26\n",
      "Agent -> thread_4, reward = -8.0, episode = 26\n",
      "Agent -> thread_3, reward = -6.0, episode = 25\n",
      "Agent -> thread_2, reward = -11.0, episode = 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-29 14:08:13,073] Starting new video recorder writing to /home/ivan/Dropbox/Faks/5_godina/nn_assignments/pong/tmp2_monitor/openaigym.video.3.8333.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent -> thread_0, reward = -3.0, episode = 26\n",
      "Agent -> thread_1, reward = -5.0, episode = 27\n",
      "Agent -> thread_4, reward = -3.0, episode = 27\n",
      "Agent -> thread_2, reward = -9.0, episode = 28\n",
      "Agent -> thread_3, reward = -4.0, episode = 26\n",
      "Agent -> thread_0, reward = -6.0, episode = 27\n",
      "Agent -> thread_1, reward = -8.0, episode = 28\n",
      "Agent -> thread_4, reward = -7.0, episode = 28\n",
      "Agent -> thread_3, reward = -7.0, episode = 27\n",
      "Agent -> thread_2, reward = -1.0, episode = 29\n",
      "Agent -> thread_1, reward = -15.0, episode = 29\n",
      "Agent -> thread_0, reward = -7.0, episode = 28\n",
      "Agent -> thread_4, reward = -9.0, episode = 29\n",
      "()\n",
      "==========\n",
      "Checkpoint Saved to tmp_model/model\n",
      "==========\n",
      "Closing threads\n",
      "Agent -> thread_2, reward = -10.0, episode = 30\n",
      "Agent -> thread_0, reward = -13.0, episode = 29\n",
      "Agent -> thread_3, reward = -6.0, episode = 28\n",
      "Agent -> thread_1, reward = -1.0, episode = 30\n",
      "Agent -> thread_4, reward = -10.0, episode = 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-29 14:10:13,493] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/ivan/Dropbox/Faks/5_godina/nn_assignments/pong/tmp2_monitor')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing environments\n"
     ]
    }
   ],
   "source": [
    "# possible_movements = np.array([1, 2, 3])\n",
    "possible_movements = np.array([1, 2, 3, 4, 5])\n",
    "output_dim = len(possible_movements)\n",
    "first_number = possible_movements[0]\n",
    "procces_image_size = [80, 80, 1]\n",
    "max_episode_number = 1000\n",
    "\n",
    "save_path = \"tmp_model/model\"\n",
    "load_path = \"dim5_model_v0/model\"\n",
    "n_threads = 5\n",
    "\n",
    "monitor = True\n",
    "monitor_name = \"tmp2_monitor\"\n",
    "\n",
    "terminate_time = 2 # Program will finish after terminate_time hours\n",
    "\n",
    "run_pong()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
