{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "**Submission deadline: last classes before 13.04.2016**\n",
    "\n",
    "**Points: 11 + 1 bonus points**\n",
    "\n",
    "\n",
    "## Downloading this notebook\n",
    "\n",
    "This assignment is an Jupyter notebook. Download it by cloning https://github.com/janchorowski/nn_assignments. Follow the instructions in its README for instructions.\n",
    "\n",
    "For programming exerciese add your solutions to the notebook. For math exercies please provide us with answers on paper or type them in the notebook i supports Latex-like equations).\n",
    "\n",
    "Please do not hesitate to use GitHubâ€™s pull requests to send us corrections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as sopt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "\n",
    "from common.gradients import check_gradient, numerical_gradient, encode_params, decode_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following assignments let:\n",
    " * $X \\in \\mathbb{R}^{k\\times N}$ be the data matrix containing $N$\n",
    "  samples each described with $k$ features. The $i$-th sample $x^{(i)} \\in\n",
    "  \\mathbb{R}^{(k\\times 1)}$ is the $i$-th column of $X$.\n",
    " * $Y \\in \\mathbb{R}^{1\\times N}$ be the row-vector of targets,\n",
    "  with $y^{(i)}$ being the target for the $i$-th sample.\n",
    " * $\\Theta\\in\\mathbb{R}^{k\\times 1}$ be the vector of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (Backpropagation through a tanh neuron) [2p]\n",
    "\n",
    "  We want to use a single neuron with the $\\tanh(x) = \\frac{e^x -\n",
    "    e^{-x}}{e^x + e^{-x}}$ activation function.\n",
    "  First find the derivative $\\frac{\\partial \\tanh(x)}{\\partial\n",
    "    x}$ and express it as a function of $\\tanh(x)$.\n",
    "  Forward computations performed by the neuron are:\n",
    "  \n",
    "  \\begin{align*}\n",
    "    A &= \\Theta^T X \\\\\n",
    "    \\hat{Y} &= \\tanh(A) \\text{ applied elementwise} \\\\\n",
    "    E &= Y - \\hat{Y} \\\\\n",
    "    J &= E \\cdot E^T\n",
    "  \\end{align*}\n",
    "  \n",
    "  Find and express using matrix notation the following gradients. You\n",
    "  can refer to values and gradients computed earlier in the expressions for the\n",
    "  following ones -- just as you would when implementing a computer\n",
    "  program. Use $\\odot$ for the elementwise multiplication of matrices.\n",
    "\n",
    "  \\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial E } &= ? \\\\\n",
    "    \\frac{\\partial J}{\\partial \\hat{Y}} &= ? \\\\\n",
    "    \\frac{\\partial J}{\\partial A} &= ? \\\\\n",
    "    \\frac{\\partial J}{\\partial \\Theta} &= ? \\\\\n",
    "  \\end{align*}\n",
    "  \n",
    "  **Note:** each gradient above should be implementable as a\n",
    "  compact expression in Python+NumPy.\n",
    "\n",
    "  **Hint:** write down the shapes of all values that you\n",
    "  compute. Work out the expressions for a single element of the\n",
    "  gradient, then see how they can be expressed using the matrix\n",
    "  notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SoftMax Regression\n",
    "\n",
    "The samples in the Iris dataset belong to one of three classes, while in\n",
    "CIFAR10 and MNIST they belong to one of 10 classes. Thus, linear regression cannot be\n",
    "applied because it distinguishes between two classes only.\n",
    "We will use SoftMax regression instead.\n",
    "\n",
    "Let $x\\in \\mathbb{R}^n$ be a sample vector and\n",
    "$y\\in \\{1,2,\\ldots,K\\}$ its class label.\n",
    "Similarly to what has been done during the lecture,\n",
    "we extend vector $x$ with the bias term $x_0=1$ to simplify the calculations\n",
    "(so now $x\\in \\mathbb{R}^{n+1}$).\n",
    "\n",
    "In SoftMax regression, we model conditional probability, that \n",
    "a given sample $x$ belongs to class $k$. Such model is parametrized\n",
    "with a matrix $\\Theta\\in\\mathbb{R}^{K \\times n+1}$.\n",
    "Note that in SoftMax regression, a separate linear model is build for each\n",
    "class. First we compute the vector $a$ of total inputs:\n",
    "\\begin{equation}\n",
    "a_k = \\sum_{j=0}^{n}\\Theta_{k,j}x_j,\n",
    "\\end{equation}\n",
    "or using matrix notation $a = \\Theta x$.\n",
    "Then we compute conditional probabilities using SoftMax regression:\n",
    "\\begin{equation}\n",
    "p(\\text{class}=k|x, \\Theta)= o_k = \\frac{\\exp{a_k}}{\\sum_{j=1}^K \\exp{a_j}}.\n",
    "\\end{equation}\n",
    "\n",
    "Function SoftMax transforms a $K$-element vector of real numbers\n",
    "to a vector of non-negative numbers which sum to 1. Thus they can be\n",
    "treated as probabilities assigned to $K$ separate classes.\n",
    "\n",
    "As it is the case with linear regression, we use cross-entropy\n",
    "as the loss function in SoftMax regression:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "J^{(i)}(\\Theta) &= - \\sum_{k=1}^{K} [y^{(i)}=k]\\log o_k^{(i)} \\\\\n",
    "J(\\Theta) &= \\frac{1}{m}\\sum_{i=1}^m J^{(i)}(\\Theta)= -\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^{K} [y^{(i)}=k]\\log o_k^{(i)} \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where $[y^{(i)}=k]$ equals $1$ when the $i-$th sample belongs to class $k$,\n",
    "and $0$ otherwise.\n",
    "Value $[y^{(i)}=k]$ might be interpreted as the correct value of the $k$-th\n",
    "output of the model on sample $i$.\n",
    "In addition, the total loss is expressed as a mean loss of particular samples,\n",
    "to make it independent of the size of the training set.\n",
    "\n",
    "Loss function gradient with respect to total inputs $a$ is simple:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J^{(i)}}{\\partial a^{(i)}_k} = o_k^{(i)} - [y^{(i)}=k].\n",
    "\\end{equation}\n",
    "\n",
    "Using the chain rule, the gradient of the loss function with respect to\n",
    "model parameters becomes:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial \\Theta_{kj}} = \\sum_{i=1}^m \\frac{\\partial J}{\\partial J^{(i)}}\\frac{\\partial J^{(i)}}{\\partial \\Theta_{kj}} = \\sum_{i=1}^m \\frac{1}{m}\\cdot \\frac{\\partial J^{(i)}}{\\partial a^{(i)}_k} \\frac{\\partial a^{(i)}_k}{\\partial \\Theta_{kj}} = \\frac{1}{m}\\sum_{i=1}^m \\frac{\\partial J^{(i)}}{\\partial a^{(i)}_k} x^{(i)}_j.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 [2p]\n",
    "  Implement SoftMax regression and apply it to the Iris dataset.\n",
    "  During training, use L-BFGS from `scipy.optimize`. You can initialize the algorithm\n",
    "  with a null matrix (all entries being zeros).\n",
    "  Obtained accuracy should be comparable with that of k-NN\n",
    "  (roughly 3% of errors).\n",
    "  If your model doesn't work, check the gradient using the `check_gradient`\n",
    "  routine from the Starter Code of Assignment 3, which computes the gradient numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 150) (5, 150)\n",
      "('IrisXFull is a 5-shaped matrix of 150', dtype('float64'))\n",
      "('IrisX2feats is a 3-shaped matrix of 150', dtype('float64'))\n",
      "('IrisY is a 1-shaped matrix of 150', dtype('int64'))\n",
      "(5, 150)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAELCAYAAADdriHjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3hG8aUaQpQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8ec0b9ad90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Here we load the IRIS dataset.\n",
    "# We will create two datasets: one using all features, and one using just Petal Langth and Petal Width for visualizations\n",
    "#\n",
    "iris = datasets.load_iris()\n",
    "petal_length = iris.data[:,iris.feature_names.index('petal length (cm)')]\n",
    "petal_width = iris.data[:, iris.feature_names.index('petal width (cm)')]\n",
    "\n",
    "IrisXFull = np.vstack([np.ones_like(petal_length), iris.data.T])\n",
    "IrisX2feats = np.vstack([np.ones_like(petal_length), petal_length, petal_width])\n",
    "print IrisX2feats.shape, IrisXFull.shape\n",
    "IrisY = iris.target.reshape(1,-1).astype(np.int64)\n",
    "\n",
    "print (\"IrisXFull is a %s-shaped matrix of %s\" % IrisXFull.shape, IrisXFull.dtype)\n",
    "print (\"IrisX2feats is a %s-shaped matrix of %s\" % IrisX2feats.shape, IrisX2feats.dtype)\n",
    "print (\"IrisY is a %s-shaped matrix of %s\" % IrisY.shape, IrisY.dtype)\n",
    "\n",
    "scatter(IrisX2feats[1,:], IrisX2feats[2,:], c=IrisY.ravel(), cmap='spring')\n",
    "xlabel('petal_length')\n",
    "ylabel('petal_width')\n",
    "print(IrisXFull.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SoftMaxRegression_implementation(ThetaFlat, X, Y=None, return_probabilities=False):\n",
    "    \"\"\"\n",
    "    Compute the outputs of a softmax classifier, or the loss and gradient\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ThetaFlat : \n",
    "        flat array of parameters containing (n_features*n_classes) entries\n",
    "    X :\n",
    "        array of features, shape n_features x n_smaples\n",
    "    Y :\n",
    "        optional array of desired targets of shape 1 x n_samples\n",
    "    return_probabilities : \n",
    "        if True, the probabilities are returned and Y is not used\n",
    "        if False, the los and gradient is computed on the X,Y pairs\n",
    "    \"\"\"\n",
    "    #X is num_features x num_samples\n",
    "    num_features, num_samples = X.shape\n",
    "\n",
    "    #Theta is num_features x num_classes\n",
    "    #we first reshape ThetaFlat into Theta\n",
    "    Theta = ThetaFlat.reshape(num_features, -1)\n",
    "\n",
    "    #Activation of softmax neurons\n",
    "    #A's shape should be num_classes x num_samples\n",
    "    #\n",
    "    # TODO \n",
    "    A = np.dot(Theta.T, X)\n",
    "    #\n",
    "    \n",
    "    #Now compute the SoftMax function\n",
    "    #O will be a num_classes x num_samples matrix of probabilities assigned by our model  \n",
    "    #Stability optimization - for each subtract the maximum activation\n",
    "    O = A - A.max(0, keepdims=True)\n",
    "#     O = A - np.max(A)\n",
    "    #\n",
    "    # TODO - compute SoftMax as vector O. Take the exp and normalize, so all values of O\n",
    "    #        would sum to 1.0.\n",
    "    O = np.exp(O)\n",
    "    O = (O/np.sum(O, axis=0))\n",
    "    \n",
    "    \n",
    "\n",
    "    if return_probabilities:\n",
    "        return O\n",
    "    \n",
    "    #The loss is the average per-sample nll (neg log likelihood)\n",
    "    #The nll is the sum of the logarithms of probabilities assigned to each class\n",
    "    correct_class_likelihoods = np.log(O[Y.ravel(), np.arange(num_samples)])\n",
    "    L = - 1.0/num_samples * np.sum(correct_class_likelihoods)\n",
    "\n",
    "    #For the softmax activation and cross-entropy loss, the derivative dNLL/dA has a simple form\n",
    "    #Please fill in its computation\n",
    "    #\n",
    "    # TODO\n",
    "#     max_elements = np.argmax(O, axis=0)\n",
    "#     O[max_elements] = O[max_elements]-1\n",
    "#     dLdA = O\n",
    "\n",
    "    dLdA = O\n",
    "    dLdA[Y, range(num_samples)] -= 1\n",
    "    dLdA /= num_samples\n",
    "    #\n",
    "\n",
    "    #Now we compute the gradient of the loss with respect to Theta\n",
    "    dLdTheta = np.dot(X, dLdA.T)\n",
    "\n",
    "    #reshape gard into the shape of Theta, for fmin_l_bfsgb to work\n",
    "    return L, dLdTheta.reshape(ThetaFlat.shape)\n",
    "\n",
    "#Make a function for training on irises\n",
    "iris_log_reg_cost = lambda Theta: SoftMaxRegression_implementation(Theta, IrisXFull, IrisY, False)\n",
    "iris_log_reg_cost2 = lambda Theta: SoftMaxRegression_implementation(Theta, IrisX2feats, IrisY, False)\n",
    "#Make sure that the gradient computation is OK\n",
    "check_gradient(iris_log_reg_cost, np.zeros((3*5,)))\n",
    "check_gradient(iris_log_reg_cost, np.random.rand(3*5)*2.0-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Call a solver\n",
    "#\n",
    "\n",
    "#iprint will cause the solver to print TO THE TERMINAL from which ipython notebook was started\n",
    "ThetaOpt = sopt.fmin_l_bfgs_b(iris_log_reg_cost, np.zeros((3*5,)), iprint=1)[0]\n",
    "\n",
    "check_gradient(iris_log_reg_cost, ThetaOpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accurracy: 98.666667%\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Compute training errors\n",
    "#\n",
    "\n",
    "probabilities = SoftMaxRegression_implementation(ThetaOpt, IrisXFull, return_probabilities=True)\n",
    "predictions = np.argmax(probabilities,0)\n",
    "\n",
    "print \"Training accurracy: %f%%\" % ((predictions==IrisY.ravel()).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accurracy: 96.000000%\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Now redo the training for two features\n",
    "#\n",
    "# TODO - again, use l_bfgs to find optimal theta, then compute probabilities and new predictions.\n",
    "#\n",
    "Theta2class = sopt.fmin_l_bfgs_b(iris_log_reg_cost2, np.zeros((3*3,)), iprint=1)[0]\n",
    "\n",
    "check_gradient(iris_log_reg_cost2, Theta2class)\n",
    "\n",
    "probabilities = SoftMaxRegression_implementation(Theta2class, IrisX2feats, return_probabilities=True)\n",
    "predictions = np.argmax(probabilities,0)\n",
    "\n",
    "print \"Training accurracy: %f%%\" % ((predictions==IrisY.ravel()).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Now plot the decision boundary\n",
    "# \n",
    "\n",
    "petal_lengths, petal_widths = np.meshgrid(np.linspace(IrisX2feats[1,:].min(), IrisX2feats[1,:].max(), 100),\n",
    "                                          np.linspace(IrisX2feats[2,:].min(), IrisX2feats[2,:].max(), 100))\n",
    "\n",
    "IrisXGrid = np.vstack([np.ones(np.prod(petal_lengths.shape)), petal_lengths.ravel(), petal_widths.ravel()])\n",
    "predictions_Grid = SoftMaxRegression_implementation(Theta2class, IrisXGrid, return_probabilities=True).argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f8ec086dd90>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEXCAYAAACpuuMDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4YLBcRzHaeL/AVIbuVxknT/+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8ec0bfcf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "contourf(petal_lengths, petal_widths, predictions_Grid.reshape(petal_lengths.shape), cmap='spring')\n",
    "scatter(IrisX2feats[1,:], IrisX2feats[2,:], c=IrisY.ravel(), cmap='spring')\n",
    "xlabel('petal_length')\n",
    "ylabel('petal_width')\n",
    "title('Decision boundary found by SoftMAx regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-layer Neural Network\n",
    "\n",
    "The task is to extend the SoftMax regression model to a 2-layer neural net.\n",
    "The network will transform an input vector to an activation vector\n",
    "of hidden neurons and finally, using the SoftMax function,\n",
    "to a vector of probabilities of the sample's belonging to one of 10 classes.\n",
    "\n",
    "To train the network, we'll need the loss function $J$ and its gradient\n",
    "with respect to network's parameters (weights and biases).\n",
    "For a 2-layer net, this can be achieved using the following relationships:\n",
    "\n",
    "### Data\n",
    "\n",
    "The training set has $m$ samples of $n$ dimensions, belonging to one\n",
    "of $K$ classes, it is given as a set of matrices: $X \\in \\mathbb{R}^{n\\times m}$\n",
    "and $Y\\in \\{1,2,\\ldots,K\\}^{1\\times m}$.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "The net will have 2 layers: 1) a hidden one, having $L$ neurons,\n",
    "and 2) an output one, having $K$ neurons (one for each of $K$ classes).\n",
    "The layers are defined through:\n",
    "\n",
    "\n",
    "1. the parameters of the hidden layer, which maps $n$-dimensional input vectors\n",
    "  into activations of $L$ neurons:\n",
    "  weight matrix $W^h\\in\\mathbb{R}^{L\\times n}$ and bias vector\n",
    "  $b^h\\in\\mathbb{R}^{L\\times 1}$,\n",
    "  \n",
    "2. the parameters of the output layer, which maps $L$-dimensional vector\n",
    "  of activations of the hidden layer to $K$ activations of output neurons:\n",
    "  weight matrix $W^o\\in{K\\times L}$ and bias vector $b^o\\in\\mathbb{R}^{K\\times 1}$.\n",
    "\n",
    "### Signal forward propagation (fprop)\n",
    "\n",
    "Each hidden neuron computes its total input as a sum of product of its\n",
    "inputs, weight matrix and bias. For an $i$-th sample,\n",
    "the total input\n",
    "${a^{h}}^{(i)}_l $ of an $I$-th neuron is thus:\n",
    "\\begin{equation}\n",
    "{a^h}^{(i)}_l = \\sum_{j=1}^n {W^h}_{l,j}x^{(i)}_j + {b^h}_l\n",
    "\\end{equation}\n",
    "The total input of neurons might also be expressed via matrices,\n",
    "using matrix multiplication and broadcasting (which allows to add\n",
    "a column vector to all column vectors of a matrix):\n",
    "\\begin{equation}\n",
    "{a^h} = W^h\\cdot x + b^h\n",
    "\\end{equation}\n",
    "This can be implemented in Python as `ah = W.dot(x) + b`.\n",
    "\n",
    "Next, we compute activation $h^h$ of hidden neurons with hyperbolic tangent\n",
    "$\\tanh(a) = \\frac{e^a-e^{-a}}{e^a+e^{-a}}$:\n",
    "\\begin{equation}\n",
    "{h^h}^{(i)}_l=\\tanh({a^h}^{(i)}_l)\n",
    "\\end{equation}\n",
    "Thanks to vectorization in Python + numpy, $h^h$ might be computed with a single\n",
    "expression `hh = numpy.tanh(ah)`.\n",
    "\n",
    "Total input of the output layer can be computed using\n",
    "activations of the hidden layer (with the help of broadcasting) as:\n",
    "\n",
    "\\begin{equation}\n",
    "a^o = W^o\\cdot h^h + b^o\n",
    "\\end{equation}\n",
    "\n",
    "Finally, probabilities of a sample's belonging to  particular classes\n",
    "have to be computed. This can be achieved with SoftMax:\n",
    "\n",
    "\\begin{equation}\n",
    "    p(y^{(i)}=k|x^{(i)}) = o^{(i)}_k = \\frac{\\exp({a^o}^{(i)}_k)}{ \\sum_{k'=1}^K \\exp( {a^o}^{(i)}_{k'} )}.\n",
    "\\end{equation}\n",
    "\n",
    "Like with SoftMax regression, we will use cross-entropy\n",
    "as the loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "J^{(i)}(\\Theta) &= - \\sum_{k=1}^{K} [y^{(i)}=k]\\log o_k^{(i)}, \\\\\n",
    "J(\\Theta) &= \\frac{1}{m}\\sum_{i=1}^m J^{(i)}(\\Theta)= -\\frac{1}{m}\\sum_{i=1}^n\\sum_{k=1}^{K} [y^{(i)}=k]\\log o_k^{(i)}.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "### Error backpropagation (bprop)\n",
    "\n",
    "Using the chain rule one can derive the gradient of the loss function\n",
    "in respect to neurons' activations and network parameters.\n",
    "\n",
    "\n",
    "First we compute the gradient with respect to the output layer's\n",
    "total inputs:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial {a^o}^{(i)}_k} = \\frac{1}{m}(o_k^{(i)} - [y^{(i)}=k]),\n",
    "\\end{equation}\n",
    "\n",
    "then we compute the gradient with respect to activations of hidden units:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial {h^h}^{(i)}_l} = \\sum_{k=1}^K \\frac{\\partial J}{\\partial {a^o}^{(i)}_k} \\frac{\\partial {a^o}^{(i)}_k}{\\partial {h^h}^{(i)}_l} =  \\sum_{k=1}^K \\frac{\\partial J}{\\partial {a^o}^{(i)}_k} {W^o}_{kl},\n",
    "\\end{equation}\n",
    "then we compute the gradient with respect to the total activations of hidden units:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial {a^h}^{(i)}_l} = \\frac{\\partial J}{\\partial {h^h}^{(i)}_l}\\frac{\\partial {h^h}^{(i)}_l}{\\partial {a^h}^{(i)}_l} = \\frac{\\partial J}{\\partial {h^h}^{(i)}_l} (1 - ({h^h}^{(i)}_l)^2),\n",
    "\\end{equation}\n",
    "\n",
    "where we have used the relationship\n",
    "\n",
    "$\\frac{\\partial \\tanh(x)}{\\partial x} = 1-\\tanh(x)^2$.\n",
    "\n",
    "Finally we can use the gradients with respect to the total inputs to\n",
    "compute the gradients with respect to network parameters,\n",
    "eg. for the input layer:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial {W^o}_{kl}} = \\sum_{i}\\frac{\\partial J}{\\partial {a^o}^{(i)}_k}\\frac{\\partial {a^o}^{(i)}_k}{\\partial {W^o}_{kl}} = \\sum_{i}\\frac{\\partial J}{\\partial {a^o}^{(i)}_k}{h^h}^{(i)}_l,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial {b^o}_{k}} = \\sum_{i}\\frac{\\partial J}{\\partial {a^o}^{(i)}_k}\\frac{\\partial {a^o}^{(i)}_k}{\\partial {b^o}_{k}} = \\sum_{i}\\frac{\\partial J}{\\partial {a^o}^{(i)}_k}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 [2p]\n",
    "  Implement a 2-layer neural network as a function\n",
    "  **TwoLayerNet($\\Theta$,X,Y)**\n",
    "  which computes the loss and gradient of loss with\n",
    "  respect to the weights and bias terms (encoded as $\\Theta$)\n",
    "  on data given as $X$ and $Y$.\n",
    "  Refer to the Starter Code below for the details.\n",
    "  Try to express as much as possible with matrix calculus.\n",
    "\n",
    "  The following problems will require to train the network.\n",
    "  Use the L-BFGS optimizer from `scipy.optimize` to minimize\n",
    "  your function (particularly: the loss) and find the right $\\Theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TwoLayerNet_implementation(ThetaFlat, ThetaShapes, X, Y=None, return_probabilities=False):\n",
    "    \"\"\"\n",
    "    Compute the outputs of a softmax classifier, or the loss and gradient\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ThetaFlat : \n",
    "        flat array of parameters\n",
    "    ThetaShapes :\n",
    "        list of shapes of weight and bias matrices\n",
    "    X :\n",
    "        array of features, shape n_features x n_smaples\n",
    "    Y :\n",
    "        optional array of desired targets of shape 1 x n_samples\n",
    "    return_probabilities : \n",
    "        if True, the probabilities are returned and Y is not used\n",
    "        if False, the los and gradient is computed on the X,Y pairs\n",
    "    \"\"\"\n",
    "    #X is num_features x num_samples\n",
    "    num_features, num_samples = X.shape\n",
    "\n",
    "    #Extract weight matrices\n",
    "    W1, W2 = decode_params(ThetaFlat, ThetaShapes)\n",
    "    \n",
    "    X_padded = np.vstack([np.ones((1, num_samples)), X])\n",
    "    \n",
    "    #Activation in first layer. Shape is num_hidden x num_samples\n",
    "    #\n",
    "    # TODO\n",
    "    A1 = np.dot(W1.T, X_padded)\n",
    "    #\n",
    "\n",
    "    #Apply the transfer function\n",
    "    #\n",
    "    # TODO Shape is num_hidden x num_samples\n",
    "    H1 = np.tanh(A1)\n",
    "    #\n",
    "        \n",
    "    #Pad with zeros\n",
    "    H1_padded = np.vstack([np.ones((1, num_samples)), H1])\n",
    "    \n",
    "    #Now apply the second linear transform\n",
    "    #\n",
    "    # TODO Shape is num_classes x num_samples\n",
    "    A2 = np.dot(W2.T, H1_padded)\n",
    "    #\n",
    "    \n",
    "    #Now compute the SoftMax function\n",
    "    #O will be a num_classes x num_samples matrix of probabilities assigned by our model  \n",
    "    #Stability optimization - for each subtract the maximum activation\n",
    "    O = A2 - A2.max(0, keepdims=True)\n",
    "    # \n",
    "    # TODO - compute SoftMax as vector O. Take the exp and normalize, so all values of O\n",
    "    #        would sum to 1.0.\n",
    "    # \n",
    "    O = np.exp(O)\n",
    "    O = (O/np.sum(O, axis=0))\n",
    "\n",
    "    if return_probabilities:\n",
    "        return O\n",
    "    \n",
    "    #The loss is the average per-sample nll (neg log likelihood)\n",
    "    #The nll is the sum of the logarithms of probabilities assigned to each class\n",
    "    correct_class_likelihoods = np.log(O[Y.ravel(), np.arange(num_samples)])\n",
    "    L = - 1.0/num_samples * np.sum(correct_class_likelihoods)\n",
    "\n",
    "    #For the softmax activation and cross-entropy loss, the derivative dNLL/dA has a simple form\n",
    "    #Please fill in its computation\n",
    "    #\n",
    "    # TODO Shape is num_classes x num_samples\n",
    "    # dLdA2 = \n",
    "    dLdA2 = O\n",
    "    dLdA2[Y, range(num_samples)] -= 1\n",
    "    dLdA2 /= num_samples\n",
    "    #\n",
    "\n",
    "    dLdH1_padded = np.dot(W2, dLdA2)\n",
    "    # Shape is num_hidden x num_samples    \n",
    "    dLdH1 = dLdH1_padded[1:,:] #ship the derivatives backpropagated to the added ones\n",
    "    #\n",
    "    # TODO - compute the derivatives dLdW2 and dLdW1\n",
    "    # Hint - to compute dLdW1, start with dLdA1\n",
    "    #\n",
    "#     dLdA1 = np.dot(1 - np.power(H1, 2), dLdH1.T)\n",
    "#     dLdA1 = (1 - np.power(H1, 2)).dot(dLdH1.T)\n",
    "\n",
    "#     dLdW2 = dLdA2.dot(H1_padded.T) prije\n",
    "    dLdW2 = H1_padded.dot(dLdA2.T)\n",
    "\n",
    "    #dLdA1 = dLdH1 * dH1dA1\n",
    "#     dLdA1 = np.dot(dLdH1.T, ((1 - np.power(H1, 2)) * X_padded.T))\n",
    "    dLdA1 = (dLdH1 * ((1 - np.power(H1, 2))))\n",
    "\n",
    "    #     dLdW1 = np.dot(dLdA1.T, X.T)\n",
    "    dLdW1 = np.dot(X, dLdA1.T)\n",
    "    \n",
    "#     dLdA1 = dH1dA1   .dot(dLdH1)\n",
    "#     dLdW1 =  dA1dW1   .dot(dLdA1)\n",
    "#     dLdA1 = (1-H1**2).dot(dLdH1)\n",
    "    dLdA1 = (1-H1**2) * (dLdH1)\n",
    "    dLdW1 = X_padded.dot(dLdA1.T)\n",
    "\n",
    "#     print dLdW1.shape, dLdW2.shape\n",
    "    \n",
    "    dLdThetaFlat, unused_shapes = encode_params([dLdW1, dLdW2])\n",
    "    \n",
    "    #reshape gard into the shape of Theta, for fmin_l_bfsgb to work\n",
    "    return L, dLdThetaFlat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 [1p] (choosing the initial vector)\n",
    "  In the cases of linear and logistic regression,\n",
    "  we could start the optimization with a vector of zeros.\n",
    "  Such initialization will be troublesome for neural networks.\n",
    "\n",
    "  You can use the following initialization methods for\n",
    "  network parameters: a) initialize weight matrices with small random\n",
    "  numbers (eg. drawn from $\\mathcal{N}(0, 0.2)$), b) initialize bias\n",
    "  vectors with zeros. Train the network on the Iris dataset and report\n",
    "  classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Here we init the network for gradient testing on IRIS\n",
    "#\n",
    "# We will have 7 hidden neurons.\n",
    "# The first weight matrix will be 5 (4 features + bias) x 7 (hidden neurons)\n",
    "# The second weight matrix will be 8 (7 neurons + bias) x 3 (classes)\n",
    "#\n",
    "num_hidden = 7\n",
    "#\n",
    "# TODO\n",
    "W1 = np.random.normal(0, 0.2, 5*7).reshape((5,7))\n",
    "W2 = np.random.normal(0, 0.2, 8*3).reshape((8,3))\n",
    "\n",
    "W12 = np.random.normal(0, 0.2, 5*7).reshape((5,7))\n",
    "W22 = np.random.normal(0, 0.2, 8*3).reshape((8,3))\n",
    "W12[4] = np.array(np.zeros(7))\n",
    "W22[7] = np.array(np.zeros(3))\n",
    "#\n",
    "\n",
    "# Now flatten into an array\n",
    "Theta0, ThetaShape = encode_params([W1,W2])\n",
    "Theta02, ThetaShape2 = encode_params([W12,W22])\n",
    "\n",
    "#Make a function for training on irises\n",
    "iris_net_cost = lambda Theta: TwoLayerNet_implementation(Theta, ThetaShape, iris.data.T, IrisY, False)\n",
    "#Make sure that the gradient computation is OK\n",
    "check_gradient(iris_net_cost, Theta0)\n",
    "check_gradient(iris_net_cost, np.zeros_like(Theta0))\n",
    "check_gradient(iris_net_cost, np.ones_like(Theta0)*0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accurracy: 99.333333%\n",
      "Training accurracy: 99.333333%\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# TODO - apply L-BFGS to minimize the loss and get optimal ThetaOpt.\n",
    "#\n",
    "ThetaOpt = sopt.fmin_l_bfgs_b(iris_net_cost, Theta0, iprint=1)[0]\n",
    "\n",
    "predictions = TwoLayerNet_implementation(ThetaOpt, ThetaShape, iris.data.T, return_probabilities=True).argmax(0)\n",
    "print \"Training accurracy: %f%%\" % ((predictions==IrisY.ravel()).mean()*100)\n",
    "\n",
    "ThetaOpt2 = sopt.fmin_l_bfgs_b(iris_net_cost, Theta02, iprint=1)[0]\n",
    "\n",
    "predictions2 = TwoLayerNet_implementation(ThetaOpt2, ThetaShape, iris.data.T, return_probabilities=True).argmax(0)\n",
    "print \"Training accurracy: %f%%\" % ((predictions2==IrisY.ravel()).mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 [1p + 1p bonus]\n",
    "\n",
    "### XOR network\n",
    "Test your network on a 2-dimensional and a 3-dimensional\n",
    "XOR problem. How many hidden neurons the network requires to express\n",
    "the XOR function?\n",
    "\n",
    "### Iris network\n",
    "Normalize the Iris dataset, so that each of the 4 attributes\n",
    "would fall into {[}-1,1{]} interval. Train the network and check classification\n",
    "accuracy.\n",
    "\n",
    "### Bonus\n",
    "Plot samples (for XOR or for Iris) in hidden neurons' activation space\n",
    "(similarly to http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/).\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XOR2X = np.array([[0,0],\n",
    "                  [0,1],\n",
    "                  [1,0],\n",
    "                  [1,1]]).T\n",
    "XOR2Y = np.array([[0,1,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For hidden number  0 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:64: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For hidden number  1 False\n",
      "For hidden number  2 False\n",
      "For hidden number  3 False\n",
      "For hidden number  4 False\n",
      "For hidden number  5 False\n",
      "For hidden number  6 False\n",
      "For hidden number  7 True\n",
      "For hidden number  8 False\n",
      "For hidden number  9 True\n",
      "For hidden number  10 False\n",
      "For hidden number  11 True\n",
      "For hidden number  12 True\n",
      "For hidden number  13 True\n",
      "For hidden number  14 True\n"
     ]
    }
   ],
   "source": [
    "for hidden in range(15):\n",
    "    \n",
    "    #init the neurons\n",
    "    num_hidden = hidden\n",
    "    result = True\n",
    "    for i in range(450):\n",
    "        W1 = (np.random.rand(3,num_hidden) - 0.5)\n",
    "        W2 = (np.random.rand(num_hidden+1,2) -0.5)\n",
    "\n",
    "        # Now flatten into an array\n",
    "        Theta0, ThetaShape = encode_params([W1,W2])\n",
    "\n",
    "        XOR2X_net_cost = lambda Theta: TwoLayerNet_implementation(Theta, ThetaShape, XOR2X, XOR2Y, False)\n",
    "        ThetaOpt = sopt.fmin_l_bfgs_b(XOR2X_net_cost, Theta0, iprint=1)[0]\n",
    "\n",
    "        result *= TwoLayerNet_implementation(ThetaOpt, ThetaShape, XOR2X, return_probabilities=True).argmax(0)\n",
    "    print \"For hidden number \", hidden, np.array_equal(result,XOR2Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For hidden number  0 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:64: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For hidden number  1 False\n",
      "For hidden number  2 False\n",
      "For hidden number  3 False\n",
      "For hidden number  4 False\n",
      "For hidden number  5 False\n",
      "For hidden number  6 False\n",
      "For hidden number  7 False\n",
      "For hidden number  8 False\n",
      "For hidden number  9 False\n",
      "For hidden number  10 True\n",
      "For hidden number  11 True\n",
      "For hidden number  12 True\n",
      "For hidden number  13 True\n",
      "For hidden number  14 True\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# TODO - repeat the experiment for 3-dimensional XOR.\n",
    "#\n",
    "XOR3X = np.array([[0,0,0],\n",
    "                  [0,0,1],\n",
    "                  [0,1,0],\n",
    "                  [0,1,1],\n",
    "                  [1,0,0],\n",
    "                  [1,0,1],\n",
    "                  [1,1,0],\n",
    "                  [1,1,1]]).T\n",
    "XOR3Y = np.array([[0,1,1,0,1,0,0,1]])\n",
    "\n",
    "for hidden in range(15):\n",
    "    \n",
    "    #init the neurons\n",
    "    num_hidden = hidden\n",
    "    result = True\n",
    "    for i in range(450):\n",
    "        W1 = (np.random.rand(4,num_hidden) - 0.5)\n",
    "        W2 = (np.random.rand(num_hidden+1,2) -0.5)\n",
    "\n",
    "        # Now flatten into an array\n",
    "        Theta0, ThetaShape = encode_params([W1,W2])\n",
    "\n",
    "        XOR3X_net_cost = lambda Theta: TwoLayerNet_implementation(Theta, ThetaShape, XOR3X, XOR3Y, False)\n",
    "        ThetaOpt = sopt.fmin_l_bfgs_b(XOR3X_net_cost, Theta0, iprint=1)[0]\n",
    "\n",
    "        result *= TwoLayerNet_implementation(ThetaOpt, ThetaShape, XOR3X, return_probabilities=True).argmax(0)\n",
    "    print \"For hidden number \", hidden, np.array_equal(result,XOR3Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accurracy: 98.666667%\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# (Bonus)\n",
    "# TODO - change network implementation code to return hidden activations.\n",
    "# Hint - locals() gives the dictionary of all objects in a functions's scope!\n",
    "#\n",
    "\n",
    "num_hidden = 7\n",
    "#\n",
    "# TODO\n",
    "W1 = np.random.normal(0, 0.2, 5*7).reshape((5,7))\n",
    "W2 = np.random.normal(0, 0.2, 8*3).reshape((8,3))\n",
    "#\n",
    "\n",
    "# Now flatten into an array\n",
    "Theta0, ThetaShape = encode_params([W1,W2])\n",
    "\n",
    "iris_net_cost = lambda Theta: TwoLayerNet_implementation(Theta, ThetaShape, iris.data.T, IrisY, False)\n",
    "ThetaOpt = sopt.fmin_l_bfgs_b(iris_net_cost, Theta0, iprint=1)[0]\n",
    "\n",
    "predictions = TwoLayerNet_implementation(ThetaOpt, ThetaShape, iris.data.T, return_probabilities=True).argmax(0)\n",
    "print \"Training accurracy: %f%%\" % ((predictions==IrisY.ravel()).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "IrisNormX = np.array(iris.data.T)\n",
    "IrisNormX -= np.mean(IrisNormX, axis = 1).reshape((-1,1))\n",
    "IrisNormX /= np.max(IrisNormX, axis=1).reshape((-1,1))\n",
    "#\n",
    "# TODO - normalize IrisNormX, so the vlaues would fall into [-1,1].\n",
    "#        Avoid looping constructs.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accurracy: 100.000000%\n"
     ]
    }
   ],
   "source": [
    "num_hidden = 10\n",
    "\n",
    "W1 = (np.random.rand(5, num_hidden) - 0.5)*0.1\n",
    "W2 = (np.random.rand(num_hidden + 1, 3) - 0.5)*0.1\n",
    "\n",
    "# Now flatten into an array\n",
    "Theta0, ThetaShape = encode_params([W1, W2])\n",
    "\n",
    "#\n",
    "# TODO - cripple and train your neural network.\n",
    "#\n",
    "\n",
    "iris_net_cost = lambda Theta: TwoLayerNet_implementation(Theta, ThetaShape, IrisNormX, IrisY, False)\n",
    "ThetaOpt = sopt.fmin_l_bfgs_b(iris_net_cost, Theta0, iprint=1)[0]\n",
    "\n",
    "\n",
    "predictions = TwoLayerNet_implementation(ThetaOpt, ThetaShape, IrisNormX, return_probabilities=True).argmax(0)\n",
    "print \"Training accurracy: %f%%\" % ((predictions==IrisY.ravel()).mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6 [1p]\n",
    "\n",
    "  Final questions:\n",
    "  \n",
    "   * Are neural networks parametric (https://en.wikipedia.org/wiki/Parametric_model) or non-parametric (https://en.wikipedia.org/wiki/Non-parametric_model) models? Why is it so?\n",
    "   \n",
    "   * What will happen if for each layer (hidden and output) all weights\n",
    "    will be initialized to the same values before the training?\n",
    "    \n",
    "   * How will the value of SoftMax function change,\n",
    "  if we will add the same constant term to each element of $a$?\n",
    "  Often, before computing SoftMax, the largest value can be subtracted\n",
    "  to mitigate large exponents and associated numerical errors.\n",
    "  Is it a good practice?\n",
    "\n",
    "   * Are two-class SoftMax regression and logistic regression equivalent (can you build a logistic regression model from a given softmax one and vice versa)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7 [1p]\n",
    "You are provided with a starter code for a modular implementation of a\n",
    "feedforward neural network. Your general task is to fill in the blanks\n",
    "in the code, and validate the implementation on the IRIS task (ake sure\n",
    "that the network can be trained on the Iris dataset to reach 100% training\n",
    "accuracy.)\n",
    "\n",
    "**Note:** the next assignment list will involve using this code, or other neural \n",
    "network implementation that you will be able to fully explain to reach a low\n",
    "error rate on the mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_in, num_out, weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(**kwargs)\n",
    "        if weight_init is None:\n",
    "            #\n",
    "            # In practice, the current recommendation is to use ReLU units and use the \n",
    "            # w = np.random.randn(n) * sqrt(2.0/n)\n",
    "            # I use normal, but difference between normal and uniform is have little impact.\n",
    "            # http://cs231n.github.io/neural-networks-2/\n",
    "            weight_init = IsotropicGaussian(mean=0., std=np.sqrt(2.0/(num_out*num_in)))\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        \n",
    "        self.W = weight_init.generate(self.rng, (num_out, num_in))\n",
    "        self.b = bias_init.generate(self.rng, (num_out, 1))\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        #Save X for later reusal\n",
    "        fprop_context = dict(X=X)\n",
    "        Y = np.dot(self.W, X) +  self.b\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        #\n",
    "        # TODO: fill in gradient computation\n",
    "        #\n",
    "        #AffineLayer\n",
    "        dLdX = np.dot(self.W.T, dLdY)\n",
    "#         dLdX = np.dot(dLdY.T, self.W)        \n",
    "        return dLdX\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        dLdW = np.dot(dLdY, X.T)\n",
    "        dLdb = dLdY.sum(1, keepdims=True)\n",
    "        return [dLdW, dLdb]\n",
    "    \n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TanhLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.tanh(X)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        #\n",
    "        # Fill in proper gradient computation\n",
    "        # TanhLayer\n",
    "        dLdX = (dLdY * (1 - Y**2)) \n",
    "#         dLdX = np.dot((1 - Y**2), dLdY.T) \n",
    "#         dLdX = np.dot(dLdY.T, (1 - Y**2)) \n",
    "        return dLdX\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReLULayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.maximum(X, 0.0)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        return dLdY * (Y>0)\n",
    "\n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def compute_probabilities(self, X):\n",
    "        O = X - X.max(axis=0, keepdims=True)\n",
    "        O = np.exp(O)\n",
    "        O /= O.sum(axis=0, keepdims=True)\n",
    "        return O\n",
    "    \n",
    "    def fprop_cost(self, X, Y):\n",
    "        NS = X.shape[1]\n",
    "        O = self.compute_probabilities(X)\n",
    "        Cost = -1.0/NS * np.log(O[Y.ravel(), range(NS)]).sum()\n",
    "        return Cost, O, dict(O=O, X=X, Y=Y)\n",
    "    \n",
    "    def bprop_cost(self, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        Y = fprop_context['Y']\n",
    "        O = fprop_context['O']\n",
    "        NS = X.shape[1]\n",
    "        dLdX = O.copy()\n",
    "        dLdX[Y, range(NS)] -= 1.0\n",
    "        dLdX /= NS\n",
    "        return dLdX\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "        return self.layers[-1].compute_probabilities(X)\n",
    "    \n",
    "    def get_cost_and_gradient(self, X, Y):\n",
    "        fp_contexts = []\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "            fp_contexts.append(fp_context)\n",
    "        \n",
    "        L, O, fp_context = self.layers[-1].fprop_cost(X, Y)\n",
    "        dLdX = self.layers[-1].bprop_cost(fp_context)\n",
    "        \n",
    "        dLdP = [] #gradient with respect to parameters\n",
    "        for i in xrange(len(self.layers)-1):\n",
    "            layer = self.layers[len(self.layers)-2-i]\n",
    "            fp_context = fp_contexts[len(self.layers)-2-i]\n",
    "            dLdP = layer.get_gradients(dLdX, fp_context) + dLdP\n",
    "            dLdX = layer.bprop(dLdX, fp_context)\n",
    "        return L, O, dLdP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training algorithms. They change the network!\n",
    "def GD(net, X, Y, alpha=1e-4, max_iters=1000000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Simple batch gradient descent\n",
    "    \"\"\"\n",
    "    old_L = np.inf\n",
    "    for i in xrange(max_iters):\n",
    "        L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "        if old_L < L:\n",
    "            print \"Iter: %d, loss increased!!\" % (i,)\n",
    "        if (old_L - L)<tolerance:\n",
    "            print \"Tolerance level reached exiting\"\n",
    "            break\n",
    "        if i % 1000 == 0:\n",
    "            err_rate = (O.argmax(0) != Y).mean()\n",
    "            print \"At iteration %d, loss %f, train error rate %f%%\" % (i, L, err_rate*100)\n",
    "        for P,G in zip(net.parameters, gradients):\n",
    "            P -= alpha * G\n",
    "        old_L = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "IrisX = iris.data.T\n",
    "IrisX = (IrisX - IrisX.mean(axis=1, keepdims=True)) / IrisX.std(axis=1, keepdims=True)\n",
    "IrisY = iris.target.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, loss 0.832070, train error rate 42.000000%\n",
      "At iteration 1000, loss 0.054075, train error rate 2.000000%\n",
      "At iteration 2000, loss 0.044271, train error rate 2.000000%\n",
      "At iteration 3000, loss 0.041721, train error rate 1.333333%\n",
      "At iteration 4000, loss 0.040584, train error rate 1.333333%\n",
      "At iteration 5000, loss 0.039828, train error rate 1.333333%\n",
      "At iteration 6000, loss 0.039179, train error rate 1.333333%\n",
      "At iteration 7000, loss 0.038586, train error rate 1.333333%\n",
      "At iteration 8000, loss 0.038060, train error rate 1.333333%\n",
      "At iteration 9000, loss 0.037605, train error rate 1.333333%\n",
      "At iteration 10000, loss 0.037212, train error rate 1.333333%\n",
      "At iteration 11000, loss 0.036863, train error rate 1.333333%\n",
      "At iteration 12000, loss 0.036538, train error rate 1.333333%\n",
      "At iteration 13000, loss 0.036218, train error rate 1.333333%\n",
      "At iteration 14000, loss 0.035891, train error rate 1.333333%\n",
      "At iteration 15000, loss 0.035547, train error rate 1.333333%\n",
      "At iteration 16000, loss 0.035181, train error rate 1.333333%\n",
      "At iteration 17000, loss 0.034790, train error rate 1.333333%\n",
      "At iteration 18000, loss 0.034372, train error rate 1.333333%\n",
      "At iteration 19000, loss 0.033927, train error rate 1.333333%\n",
      "At iteration 20000, loss 0.033455, train error rate 1.333333%\n",
      "At iteration 21000, loss 0.032955, train error rate 1.333333%\n",
      "At iteration 22000, loss 0.032426, train error rate 1.333333%\n",
      "At iteration 23000, loss 0.031867, train error rate 1.333333%\n",
      "At iteration 24000, loss 0.031273, train error rate 1.333333%\n",
      "At iteration 25000, loss 0.030636, train error rate 1.333333%\n",
      "At iteration 26000, loss 0.029947, train error rate 1.333333%\n",
      "At iteration 27000, loss 0.029192, train error rate 1.333333%\n",
      "At iteration 28000, loss 0.028350, train error rate 1.333333%\n",
      "At iteration 29000, loss 0.027395, train error rate 1.333333%\n",
      "At iteration 30000, loss 0.026285, train error rate 1.333333%\n",
      "At iteration 31000, loss 0.024978, train error rate 1.333333%\n",
      "At iteration 32000, loss 0.023457, train error rate 0.666667%\n",
      "At iteration 33000, loss 0.021758, train error rate 0.666667%\n",
      "At iteration 34000, loss 0.019932, train error rate 0.666667%\n",
      "At iteration 35000, loss 0.018020, train error rate 0.666667%\n",
      "At iteration 36000, loss 0.016062, train error rate 0.666667%\n",
      "At iteration 37000, loss 0.014135, train error rate 0.666667%\n",
      "At iteration 38000, loss 0.012353, train error rate 0.000000%\n",
      "At iteration 39000, loss 0.010796, train error rate 0.000000%\n",
      "At iteration 40000, loss 0.009482, train error rate 0.000000%\n",
      "At iteration 41000, loss 0.008389, train error rate 0.000000%\n",
      "At iteration 42000, loss 0.007481, train error rate 0.000000%\n",
      "At iteration 43000, loss 0.006721, train error rate 0.000000%\n",
      "At iteration 44000, loss 0.006079, train error rate 0.000000%\n",
      "At iteration 45000, loss 0.005528, train error rate 0.000000%\n",
      "At iteration 46000, loss 0.005050, train error rate 0.000000%\n",
      "At iteration 47000, loss 0.004632, train error rate 0.000000%\n",
      "At iteration 48000, loss 0.004261, train error rate 0.000000%\n",
      "At iteration 49000, loss 0.003930, train error rate 0.000000%\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we verify that the network can be trained on Irises.\n",
    "# Most runs should result in 100% accurracy\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(4,10),\n",
    "        TanhLayer(),\n",
    "        AffineLayer(10,3),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "GD(net, IrisX,IrisY, 1e-1, tolerance=1e-7, max_iters=50000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
